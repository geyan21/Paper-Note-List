# Reinforcement Learning

- [Reinforcement Learning](#reinforcement-learning)
  - [Sample Efficiency](#sample-efficiency)
  - [Offline RL](#offline-rl)
  - [Imitation Learning](#imitation-learning)
  - [Model-based RL](#model-based-rl)
  - [Multi-Task RL](#multi-task-rl)
  - [Language](#language)

## Sample Efficiency
- ICLR 2023, [Sample-Efficient Reinforcement Learning by Breaking the Replay Ratio Barrier](https://openreview.net/forum?id=OpC-9aBBVJe)
- ICLR 2023, [Hybrid RL: Using both offline and online data can make RL efficient](https://openreview.net/forum?id=yyBis80iUuU)
- arXiv 2023, RLPD: [Efficient Online Reinforcement Learning with Offline Data](https://arxiv.org/abs/2302.02948)
- ICLR 2021, REDQ: [Randomized Ensembled Double Q-Learning: Learning Fast Without a Model](https://arxiv.org/abs/2101.05982)
- NIPS 2017, HER: [Hindsight Experience Replay](https://arxiv.org/abs/1707.01495)


## Offline RL
- ICLR 2023, **X-QL**: Extreme Q-Learning: MaxEnt RL without Entropy, [Website](https://div99.github.io/XQL/)
- ICLR 2023, **Diffusion-QL**: Diffusion Policies as an Expressive Policy Class for Offline Reinforcement Learning, [OpenReview](https://openreview.net/forum?id=AHvFDPi-FA)
- ICLR 2022, **IQL**: Offline Reinforcement Learning with Implicit Q-Learning, [arXiv](https://arxiv.org/abs/2110.06169)
- NIPS 2021, **Decision Transformer**: Reinforcement Learning via Sequence Modeling, [Website](https://sites.google.com/berkeley.edu/decision-transformer)
- NIPS 2020, **CQL**: Conservative Q-Learning for Offline Reinforcement Learning, [Website](https://proceedings.neurips.cc/paper/2020/hash/0d2b2061826a5df3221116a5085a6052-Abstract.html)
- ICLR 2021 rejection, D4RL: [Datasets for Deep Data-Driven Reinforcement Learning](https://sites.google.com/view/d4rl/home)

## Imitation Learning
- NIPS 2016, GAIL: [Generative adversarial imitation learning](https://proceedings.neurips.cc/paper/2016/hash/cc7e2b878868cbae992d1fb743995d8f-Abstract.html)

## Model-based RL
- arXiv 2023, [Predictable MDP Abstraction for Unsupervised Model-Based RL](https://seohong.me/projects/pma/)
- ICML 2022, TD-MPC: [Temporal Difference Learning for Model Predictive Control](https://arxiv.org/abs/2203.04955)

## Multi-Task RL
- ICML 2021, CARE: [Multi-Task Reinforcement Learning with Context-based Representations](https://arxiv.org/abs/2102.06177)
- NIPS 2020, [Multi-Task Reinforcement Learning with Soft Modularization](https://rchalyang.github.io/SoftModule/)


## Language
- arXiv 2022, [Can Wikipedia Help Offline Reinforcement Learning?](https://arxiv.org/abs/2201.12122)
- arXiv 2023, [The Wisdom of Hindsight Makes Language Models Better Instruction Followers](https://arxiv.org/abs/2302.05206)
- NIPS 2022, [Pre-Trained Language Models for Interactive Decision-Making](https://arxiv.org/abs/2202.01771)
- CoRL 2022, SayCan: [Do As I Can, Not As I Say: Grounding Language in Robotic Affordances](https://say-can.github.io/)
